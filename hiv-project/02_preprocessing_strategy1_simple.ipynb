{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Strategy 1: Simple Imputation\n",
    "## Approach: Mean/Median for numeric features, Mode for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:14.171207Z",
     "iopub.status.busy": "2025-11-19T01:42:14.170996Z",
     "iopub.status.idle": "2025-11-19T01:42:18.961394Z",
     "shell.execute_reply": "2025-11-19T01:42:18.960733Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:18.963759Z",
     "iopub.status.busy": "2025-11-19T01:42:18.963530Z",
     "iopub.status.idle": "2025-11-19T01:42:19.035457Z",
     "shell.execute_reply": "2025-11-19T01:42:19.034963Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (45920, 48)\n",
      "Target variable distribution:\n",
      "undetectable\n",
      "0.0    21947\n",
      "1.0    11064\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('clinical_genotype_HGB.csv')\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Target variable distribution:\\n{df['undetectable'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection and Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.051565Z",
     "iopub.status.busy": "2025-11-19T01:42:19.051453Z",
     "iopub.status.idle": "2025-11-19T01:42:19.053673Z",
     "shell.execute_reply": "2025-11-19T01:42:19.053424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected: 32\n",
      "\n",
      "Features: ['status', 'visit', 'race', 'anydrug', 'ageatvis', 'nrti', 'nnrti', 'pi', 'hemoglob', 'n', 'call', 'genotype', 'logvl', 'sqrtcd4', 'sqrtcd8', 'duration', 'durationy', 'cd8a', 'vla', 'genotype3', 'CD4_8', 'APOBEC', 'APOB', 'APOBgr', 'Hgb', 'Hgbgen', 'HgbgenSS', 'apofer', 'ferss', 'aposs', 'APOBgr2', 'N']\n"
     ]
    }
   ],
   "source": [
    "# Define features to exclude (identifiers, dates, target, redundant)\n",
    "exclude_features = [\n",
    "    'wihsid', 'bsdate', 'bsvisit', 'dob', 'date',  # Identifiers and dates\n",
    "    'lnegdate', 'fposdate', 'frstartd', 'frstaidd', 'frstdthd',  # Date features\n",
    "    'undetectable',  # Target variable\n",
    "    'HIV',  # Redundant with target\n",
    "    'r',  # Reference variable\n",
    "    'vload',  # Use logvl instead\n",
    "    'CD4N',  # Use sqrtcd4 instead\n",
    "    'CD8N',  # Use sqrtcd8 instead\n",
    "]\n",
    "\n",
    "# Select all features except excluded ones\n",
    "feature_cols = [col for col in df.columns if col not in exclude_features]\n",
    "\n",
    "print(f\"Number of features selected: {len(feature_cols)}\")\n",
    "print(f\"\\nFeatures: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.054738Z",
     "iopub.status.busy": "2025-11-19T01:42:19.054656Z",
     "iopub.status.idle": "2025-11-19T01:42:19.065068Z",
     "shell.execute_reply": "2025-11-19T01:42:19.064830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape after removing missing targets: (33011, 32)\n",
      "Target distribution:\n",
      "undetectable\n",
      "0.0    21947\n",
      "1.0    11064\n",
      "Name: count, dtype: int64\n",
      "Class balance: undetectable\n",
      "0.0    0.664839\n",
      "1.0    0.335161\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df[feature_cols].copy()\n",
    "y = df['undetectable'].copy()\n",
    "\n",
    "# Remove rows where target is missing\n",
    "mask = y.notna()\n",
    "X = X[mask]\n",
    "y = y[mask]\n",
    "\n",
    "print(f\"Data shape after removing missing targets: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
    "print(f\"Class balance: {y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.066313Z",
     "iopub.status.busy": "2025-11-19T01:42:19.066217Z",
     "iopub.status.idle": "2025-11-19T01:42:19.069914Z",
     "shell.execute_reply": "2025-11-19T01:42:19.069662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features (23): ['status', 'visit', 'race', 'anydrug', 'ageatvis', 'nrti', 'nnrti', 'pi', 'hemoglob', 'n', 'logvl', 'sqrtcd4', 'sqrtcd8', 'duration', 'durationy', 'cd8a', 'vla', 'CD4_8', 'apofer', 'ferss', 'aposs', 'APOBgr2', 'N']\n",
      "\n",
      "Categorical features (9): ['call', 'genotype', 'genotype3', 'APOBEC', 'APOB', 'APOBgr', 'Hgb', 'Hgbgen', 'HgbgenSS']\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Split Data (Before Imputation to Prevent Data Leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.070998Z",
     "iopub.status.busy": "2025-11-19T01:42:19.070932Z",
     "iopub.status.idle": "2025-11-19T01:42:19.080132Z",
     "shell.execute_reply": "2025-11-19T01:42:19.079895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (26408, 32)\n",
      "Test set: (6603, 32)\n",
      "\n",
      "Training set class distribution:\n",
      "undetectable\n",
      "0.0    0.664836\n",
      "1.0    0.335164\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set class distribution:\n",
      "undetectable\n",
      "0.0    0.664849\n",
      "1.0    0.335151\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTraining set class distribution:\\n{y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\nTest set class distribution:\\n{y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple Imputation Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.081289Z",
     "iopub.status.busy": "2025-11-19T01:42:19.081199Z",
     "iopub.status.idle": "2025-11-19T01:42:19.110305Z",
     "shell.execute_reply": "2025-11-19T01:42:19.110016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features imputed with median\n",
      "Training set missing values after imputation: 0\n",
      "Test set missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Strategy: Median for numeric (robust to outliers), Most frequent for categorical\n",
    "\n",
    "# Numeric imputation with median\n",
    "numeric_imputer = SimpleImputer(strategy='median')\n",
    "X_train_numeric = numeric_imputer.fit_transform(X_train[numeric_features])\n",
    "X_test_numeric = numeric_imputer.transform(X_test[numeric_features])\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_numeric = pd.DataFrame(X_train_numeric, columns=numeric_features, index=X_train.index)\n",
    "X_test_numeric = pd.DataFrame(X_test_numeric, columns=numeric_features, index=X_test.index)\n",
    "\n",
    "print(f\"Numeric features imputed with median\")\n",
    "print(f\"Training set missing values after imputation: {X_train_numeric.isnull().sum().sum()}\")\n",
    "print(f\"Test set missing values after imputation: {X_test_numeric.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.114509Z",
     "iopub.status.busy": "2025-11-19T01:42:19.114191Z",
     "iopub.status.idle": "2025-11-19T01:42:19.142310Z",
     "shell.execute_reply": "2025-11-19T01:42:19.141975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features imputed with most frequent value\n",
      "Training set missing values after imputation: 0\n",
      "Test set missing values after imputation: 0\n"
     ]
    }
   ],
   "source": [
    "# Categorical imputation with most frequent value\n",
    "if len(categorical_features) > 0:\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    X_train_categorical = categorical_imputer.fit_transform(X_train[categorical_features])\n",
    "    X_test_categorical = categorical_imputer.transform(X_test[categorical_features])\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    X_train_categorical = pd.DataFrame(X_train_categorical, columns=categorical_features, index=X_train.index)\n",
    "    X_test_categorical = pd.DataFrame(X_test_categorical, columns=categorical_features, index=X_test.index)\n",
    "    \n",
    "    print(f\"Categorical features imputed with most frequent value\")\n",
    "    print(f\"Training set missing values after imputation: {X_train_categorical.isnull().sum().sum()}\")\n",
    "    print(f\"Test set missing values after imputation: {X_test_categorical.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"No categorical features to impute\")\n",
    "    X_train_categorical = pd.DataFrame(index=X_train.index)\n",
    "    X_test_categorical = pd.DataFrame(index=X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.143495Z",
     "iopub.status.busy": "2025-11-19T01:42:19.143417Z",
     "iopub.status.idle": "2025-11-19T01:42:19.165781Z",
     "shell.execute_reply": "2025-11-19T01:42:19.165519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features encoded\n",
      "Label encoders created for: ['call', 'genotype', 'genotype3', 'APOBEC', 'APOB', 'APOBgr', 'Hgb', 'Hgbgen', 'HgbgenSS']\n"
     ]
    }
   ],
   "source": [
    "# Label encode categorical features\n",
    "if len(categorical_features) > 0:\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        le = LabelEncoder()\n",
    "        X_train_categorical[col] = le.fit_transform(X_train_categorical[col].astype(str))\n",
    "        X_test_categorical[col] = le.transform(X_test_categorical[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    print(f\"Categorical features encoded\")\n",
    "    print(f\"Label encoders created for: {list(label_encoders.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Combine Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.166912Z",
     "iopub.status.busy": "2025-11-19T01:42:19.166843Z",
     "iopub.status.idle": "2025-11-19T01:42:19.170624Z",
     "shell.execute_reply": "2025-11-19T01:42:19.170400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape after imputation: (26408, 32)\n",
      "Test set shape after imputation: (6603, 32)\n",
      "\n",
      "Missing values in training set: 0\n",
      "Missing values in test set: 0\n"
     ]
    }
   ],
   "source": [
    "# Combine numeric and categorical features\n",
    "X_train_imputed = pd.concat([X_train_numeric, X_train_categorical], axis=1)\n",
    "X_test_imputed = pd.concat([X_test_numeric, X_test_categorical], axis=1)\n",
    "\n",
    "print(f\"Training set shape after imputation: {X_train_imputed.shape}\")\n",
    "print(f\"Test set shape after imputation: {X_test_imputed.shape}\")\n",
    "print(f\"\\nMissing values in training set: {X_train_imputed.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in test set: {X_test_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.171706Z",
     "iopub.status.busy": "2025-11-19T01:42:19.171636Z",
     "iopub.status.idle": "2025-11-19T01:42:19.197892Z",
     "shell.execute_reply": "2025-11-19T01:42:19.197655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled using StandardScaler\n",
      "\n",
      "Scaled training set statistics:\n",
      "        status         visit          race       anydrug      ageatvis  \\\n",
      "count  26408.0  2.640800e+04  2.640800e+04  2.640800e+04  2.640800e+04   \n",
      "mean       0.0 -1.300922e-16 -1.115537e-15  6.322991e-18 -4.533719e-16   \n",
      "std        0.0  1.000019e+00  1.000019e+00  1.000019e+00  1.000019e+00   \n",
      "min        0.0 -1.657450e+00 -1.953785e-01 -5.838218e-01 -2.848083e+00   \n",
      "25%        0.0 -8.870137e-01 -1.953785e-01 -5.838218e-01 -7.051993e-01   \n",
      "50%        0.0 -3.097343e-02 -1.953785e-01 -5.838218e-01 -2.725891e-02   \n",
      "75%        0.0  8.250668e-01 -1.953785e-01  1.712851e+00  6.736997e-01   \n",
      "max        0.0  1.681107e+00  5.118272e+00  1.712851e+00  3.791020e+00   \n",
      "\n",
      "               nrti         nnrti            pi      hemoglob             n  \\\n",
      "count  2.640800e+04  2.640800e+04  2.640800e+04  2.640800e+04  2.640800e+04   \n",
      "mean   1.167735e-16  5.603246e-17  4.365554e-17 -1.379757e-15 -8.354420e-17   \n",
      "std    1.000019e+00  1.000019e+00  1.000019e+00  1.000019e+00  1.000019e+00   \n",
      "min   -1.306300e+00 -5.592282e-01 -6.985469e-01 -3.903136e+00 -1.286773e+00   \n",
      "25%   -1.306300e+00 -5.592282e-01 -6.985469e-01 -8.830291e-02 -8.919885e-01   \n",
      "50%    5.012333e-01 -5.592282e-01 -6.985469e-01 -8.830291e-02 -2.011149e-01   \n",
      "75%    5.012333e-01 -5.592282e-01  5.382769e-01 -8.830291e-02  6.871512e-01   \n",
      "max    4.116301e+00  6.326737e+00  6.722396e+00  2.071987e+01  2.661076e+00   \n",
      "\n",
      "       ...             N          call      genotype     genotype3  \\\n",
      "count  ...  2.640800e+04  2.640800e+04  2.640800e+04  2.640800e+04   \n",
      "mean   ... -1.243073e-16  1.388367e-16  1.668193e-16  1.388367e-16   \n",
      "std    ...  1.000019e+00  1.000019e+00  1.000019e+00  1.000019e+00   \n",
      "min    ... -2.333199e+00 -9.173747e+00 -4.902171e+00 -9.173747e+00   \n",
      "25%    ... -6.329310e-01  1.987503e-01  2.039913e-01  1.987503e-01   \n",
      "50%    ... -9.600420e-02  1.987503e-01  2.039913e-01  1.987503e-01   \n",
      "75%    ...  9.778494e-01  1.987503e-01  2.039913e-01  1.987503e-01   \n",
      "max    ...  1.246313e+00  1.987503e-01  2.039913e-01  1.987503e-01   \n",
      "\n",
      "             APOBEC          APOB        APOBgr           Hgb        Hgbgen  \\\n",
      "count  2.640800e+04  2.640800e+04  2.640800e+04  2.640800e+04  2.640800e+04   \n",
      "mean   2.475384e-17 -5.192924e-17 -5.192924e-17  2.152508e-18  4.412640e-17   \n",
      "std    1.000019e+00  1.000019e+00  1.000019e+00  1.000019e+00  1.000019e+00   \n",
      "min   -2.042117e-01 -2.056049e-01 -2.056049e-01 -1.628672e-01 -1.505132e-01   \n",
      "25%   -2.042117e-01 -2.056049e-01 -2.056049e-01 -1.628672e-01 -1.505132e-01   \n",
      "50%   -2.042117e-01 -2.056049e-01 -2.056049e-01 -1.628672e-01 -1.505132e-01   \n",
      "75%   -2.042117e-01 -2.056049e-01 -2.056049e-01 -1.628672e-01 -1.505132e-01   \n",
      "max    1.065382e+01  8.997130e+00  8.997130e+00  1.323588e+01  6.643936e+00   \n",
      "\n",
      "           HgbgenSS  \n",
      "count  2.640800e+04  \n",
      "mean  -5.919396e-18  \n",
      "std    1.000019e+00  \n",
      "min   -1.636905e-01  \n",
      "25%   -1.636905e-01  \n",
      "50%   -1.636905e-01  \n",
      "75%   -1.636905e-01  \n",
      "max    8.870246e+00  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Standardize features (important for logistic regression and neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train_imputed.columns, index=X_train_imputed.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_imputed.columns, index=X_test_imputed.index)\n",
    "\n",
    "print(f\"Features scaled using StandardScaler\")\n",
    "print(f\"\\nScaled training set statistics:\")\n",
    "print(X_train_scaled.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.199120Z",
     "iopub.status.busy": "2025-11-19T01:42:19.199030Z",
     "iopub.status.idle": "2025-11-19T01:42:19.670546Z",
     "shell.execute_reply": "2025-11-19T01:42:19.670292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to 'preprocessed_data/' directory\n",
      "  - strategy1_X_train.csv: (26408, 32)\n",
      "  - strategy1_X_test.csv: (6603, 32)\n",
      "  - strategy1_y_train.csv: (26408,)\n",
      "  - strategy1_y_test.csv: (6603,)\n",
      "  - strategy1_preprocessing_objects.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed data and preprocessing objects\n",
    "import pickle\n",
    "\n",
    "# Create directory for preprocessed data\n",
    "import os\n",
    "os.makedirs('preprocessed_data', exist_ok=True)\n",
    "\n",
    "# Save data\n",
    "X_train_scaled.to_csv('preprocessed_data/strategy1_X_train.csv', index=False)\n",
    "X_test_scaled.to_csv('preprocessed_data/strategy1_X_test.csv', index=False)\n",
    "y_train.to_csv('preprocessed_data/strategy1_y_train.csv', index=False, header=['undetectable'])\n",
    "y_test.to_csv('preprocessed_data/strategy1_y_test.csv', index=False, header=['undetectable'])\n",
    "\n",
    "# Save preprocessing objects\n",
    "preprocessing_objects = {\n",
    "    'numeric_imputer': numeric_imputer,\n",
    "    'categorical_imputer': categorical_imputer if len(categorical_features) > 0 else None,\n",
    "    'label_encoders': label_encoders if len(categorical_features) > 0 else None,\n",
    "    'scaler': scaler,\n",
    "    'feature_cols': feature_cols,\n",
    "    'numeric_features': numeric_features,\n",
    "    'categorical_features': categorical_features\n",
    "}\n",
    "\n",
    "with open('preprocessed_data/strategy1_preprocessing_objects.pkl', 'wb') as f:\n",
    "    pickle.dump(preprocessing_objects, f)\n",
    "\n",
    "print(\"Preprocessed data saved to 'preprocessed_data/' directory\")\n",
    "print(f\"  - strategy1_X_train.csv: {X_train_scaled.shape}\")\n",
    "print(f\"  - strategy1_X_test.csv: {X_test_scaled.shape}\")\n",
    "print(f\"  - strategy1_y_train.csv: {y_train.shape}\")\n",
    "print(f\"  - strategy1_y_test.csv: {y_test.shape}\")\n",
    "print(f\"  - strategy1_preprocessing_objects.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:42:19.671786Z",
     "iopub.status.busy": "2025-11-19T01:42:19.671708Z",
     "iopub.status.idle": "2025-11-19T01:42:19.676265Z",
     "shell.execute_reply": "2025-11-19T01:42:19.676006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PREPROCESSING STRATEGY 1 - SIMPLE IMPUTATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "1. Imputation Strategy:\n",
      "   - Numeric features: Median imputation\n",
      "   - Categorical features: Most frequent value imputation\n",
      "\n",
      "2. Data Splits:\n",
      "   - Training set: 26,408 samples (80.0%)\n",
      "   - Test set: 6,603 samples (20.0%)\n",
      "\n",
      "3. Features:\n",
      "   - Total features: 32\n",
      "   - Numeric features: 23\n",
      "   - Categorical features: 9\n",
      "\n",
      "4. Target Distribution (Training):\n",
      "   - Suppressed (1): 8,851 (33.52%)\n",
      "   - Not Suppressed (0): 17,557 (66.48%)\n",
      "   - Class imbalance ratio: 1.98:1\n",
      "\n",
      "5. Scaling:\n",
      "   - Method: StandardScaler (mean=0, std=1)\n",
      "\n",
      "6. Data Quality:\n",
      "   - Missing values in training: 0\n",
      "   - Missing values in test: 0\n",
      "   - Infinite values in training: 0\n",
      "   - Infinite values in test: 0\n",
      "\n",
      "======================================================================\n",
      "Data ready for modeling!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PREPROCESSING STRATEGY 1 - SIMPLE IMPUTATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. Imputation Strategy:\")\n",
    "print(f\"   - Numeric features: Median imputation\")\n",
    "print(f\"   - Categorical features: Most frequent value imputation\")\n",
    "\n",
    "print(f\"\\n2. Data Splits:\")\n",
    "print(f\"   - Training set: {X_train_scaled.shape[0]:,} samples ({X_train_scaled.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"   - Test set: {X_test_scaled.shape[0]:,} samples ({X_test_scaled.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n3. Features:\")\n",
    "print(f\"   - Total features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"   - Numeric features: {len(numeric_features)}\")\n",
    "print(f\"   - Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "print(f\"\\n4. Target Distribution (Training):\")\n",
    "print(f\"   - Suppressed (1): {(y_train==1).sum():,} ({(y_train==1).mean()*100:.2f}%)\")\n",
    "print(f\"   - Not Suppressed (0): {(y_train==0).sum():,} ({(y_train==0).mean()*100:.2f}%)\")\n",
    "print(f\"   - Class imbalance ratio: {(y_train==0).sum()/(y_train==1).sum():.2f}:1\")\n",
    "\n",
    "print(f\"\\n5. Scaling:\")\n",
    "print(f\"   - Method: StandardScaler (mean=0, std=1)\")\n",
    "\n",
    "print(f\"\\n6. Data Quality:\")\n",
    "print(f\"   - Missing values in training: {X_train_scaled.isnull().sum().sum()}\")\n",
    "print(f\"   - Missing values in test: {X_test_scaled.isnull().sum().sum()}\")\n",
    "print(f\"   - Infinite values in training: {np.isinf(X_train_scaled.values).sum()}\")\n",
    "print(f\"   - Infinite values in test: {np.isinf(X_test_scaled.values).sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Data ready for modeling!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
